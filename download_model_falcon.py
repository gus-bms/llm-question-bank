from huggingface_hub import snapshot_download
from dotenv import load_dotenv
import os

def download_model():
    """Falcon-7B-Instruct ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    print("ğŸ“¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë”©
    load_dotenv()
    token = os.getenv("HUGGING_FACE_HUB_TOKEN")

    if not token:
        print("âŒ í™˜ê²½ ë³€ìˆ˜ 'HUGGING_FACE_HUB_TOKEN'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    repo_id = "tiiuae/falcon-7b-instruct"
    local_dir = "/mnt/storage/models/falcon-7b-instruct"

    try:
        model_dir = snapshot_download(
            repo_id=repo_id,
            local_dir=local_dir,
            local_dir_use_symlinks=False,
            token=token
        )
        print(f"âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {model_dir}")
        return model_dir
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

if __name__ == "__main__":
    download_model()

# from huggingface_hub import hf_hub_download
# import os

# def download_model():
#     """Meta-Llama-3-8B-Instruct GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
#     print("ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
#     # ëª¨ë¸ ì •ë³´
#     repo_id = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF"
#     filename = "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
#     local_dir = "models"
    
#     try:
#         # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
#         model_path = hf_hub_download(
#             repo_id=repo_id,
#             filename=filename,
#             local_dir=local_dir,
#             local_dir_use_symlinks=False
#         )
#         print(f"ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {model_path}")
#         return model_path
#     except Exception as e:
#         print(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
#         return None

# if __name__ == "__main__":
#     download_model() 